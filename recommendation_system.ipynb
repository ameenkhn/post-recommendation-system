import pandas as pd
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import train_test_split, cross_validate

# Load datasets
posts = pd.read_csv('/content/post_data.csv')
users = pd.read_csv('/content/user_data.csv')
views = pd.read_csv('/content/view_data.csv')

# Display first few rows of the data for better understanding
print("Here's a preview of the dataset:")
print(posts.head())
print(users.head())
print(views.head())

# Data preprocessing
posts.dropna(inplace=True)
users.dropna(inplace=True)
views.dropna(inplace=True)
views['time_stamp'] = pd.to_datetime(views['time_stamp'])
views['rating'] = 1  # Assume a binary rating (viewed = 1)

# Convert views data into a format suitable for Surprise
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(views[['user_id', 'post_id', 'rating']], reader)

# Split the data into training and testing sets
trainset, testset = train_test_split(data, test_size=0.2)

# Use SVD for collaborative filtering
algo = SVD()
algo.fit(trainset)

# Test and evaluate
predictions = algo.test(testset)
accuracy.rmse(predictions)
accuracy.mae(predictions)

# Cross-validate
cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)

# Function to get top N recommendations for a user
def get_top_n_recommendations(user_id, n=10):
    post_ids = posts['post_id'].unique()
    predictions = [algo.predict(user_id, post_id) for post_id in post_ids]
    predictions.sort(key=lambda x: x.est, reverse=True)
    top_n = [pred.iid for pred in predictions[:n]]
    return top_n

# Example: Get top 10 recommendations for user with user_id=1
recommended_posts = get_top_n_recommendations(1, 10)
print(recommended_posts)
print(posts[posts['post_id'].isin(recommended_posts)])
